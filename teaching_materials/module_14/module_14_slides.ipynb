{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xom1o-ebeBv3",
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Session 14: Text as Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Required readings\n",
    "\n",
    "- Gentzkow, M., Kelly, B.T. and Taddy, M., 2019. [\"Text as data\"](https://doi.org/10.1257/jel.20181020) *Journal of Economic Literature* 57(3).\n",
    "  - Following sections:\n",
    "    - 1. Introduction\n",
    "    - 2. Representing Text as Data\n",
    "\n",
    "- Chapter 2. Dan Jurafsky and James H. Martin: [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "  - Following sections:\n",
    "    - 2.4 Text Normalization\n",
    "\n",
    "- PML; Python Machine Learning, 3rd ed. (2019) by Sebastian Raschka & Vahid Mirjalili: following sections from chapter 8:\n",
    "  - Introduction\n",
    "  - Preparing the IMDb movie review data for text processing\n",
    "  - Introducing the bag-of-words model\n",
    "  - Training a logistic regression model for document classification\n",
    "  - Topic modeling with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of Session 14\n",
    "\n",
    "1. **Intro to text as data**\n",
    "2. **Examples of text as data for social scientists**\n",
    "3. **What is a text?**\n",
    "    - What do we mean by a \"document\"?\n",
    "    - We need to represent the words of a text in a structured way!\n",
    "4. **A text data analysis recipe**\n",
    "    1. Specify your document\n",
    "    2. Preprocess your text\n",
    "    3. Apply\n",
    "5. **Cleaning and preprocessing text**\n",
    "    - Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "    - Tokenization and stop-words\n",
    "    - Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "6. **Bag of Words model**\n",
    "    - Term frequency\n",
    "    - N-grams\n",
    "    - Term frequency - Inverse Document Frequency\n",
    "7. **Applications:**\n",
    "    1. **Training a logistic model to classify whether a text is positive or negative**\n",
    "        - IMDB reviews\n",
    "    2. **Lexicons**\n",
    "        - Is a word positive or negative?\n",
    "    3. **Topic modelling**\n",
    "        - Assign topics to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Intro to text as data\n",
    "\n",
    "Regard this session as an appetizer!\n",
    "- Text as data can be a course in itself\n",
    "- We cannot go into details, so don't worry if you do not understand everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use the session as an overview of what text analysis can do\n",
    "    - What do you find interesting? Dive into the details yourself\n",
    "    - Maybe already in the exam project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Want to work with text as data?\n",
    "    - Good starting point is PML chapter 8!\n",
    "        - Nice and easily accessible introduction to text data analysis\n",
    "        - Read it carefully\n",
    "            - There are many steps in text data analysis\n",
    "            - If you miss one step, the other steps might be hard to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Examples of text as data for social scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Examples you have already seen in the course\n",
    "\n",
    "- News paper articles\n",
    "- Job posts\n",
    "- Reviews on Trustpilot (quick example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Other examples\n",
    "\n",
    "- Social media (tweets, Facebook posts etc.)\n",
    "- Text from central bank reports: https://sekhansen.github.io/pdf_files/jme_2019.pdf \n",
    "- Text from annual reports: https://www.nationalbanken.dk/da/publikationer/Documents/2018/11/WP_130.pdf\n",
    "- Congressional speeches and partisanship in the US: https://scholar.harvard.edu/files/shapiro/files/politext.pdf \n",
    "- Property descriptions on property portals\n",
    "- AirBnB descriptions\n",
    "- Can you find more examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project ideas\n",
    "\n",
    "- Predicting election outcomes or market trends from sentiment\n",
    "- Stance or sentiment towards political parties\n",
    "- Hate speech detection\n",
    "- Analysing the most important topics in a public debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. What is a text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## A dataset of movie reviews and sentiment towards the movies\n",
    "\n",
    "*(Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*\n",
    "\n",
    "*Data from http://ai.stanford.edu/~amaas/data/sentiment/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Towards the end of the movie, I felt it was to...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>This is the kind of movie that my enemies cont...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I saw 'Descent' last night at the Stockholm Fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>This is one of the dumbest films, I've ever se...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment    set\n",
       "0      I went and saw this movie last night after bei...          1   test\n",
       "1      Actor turned director Bill Paxton follows up h...          1   test\n",
       "2      As a recreational golfer with some knowledge o...          1   test\n",
       "3      I saw this film in a sneak preview, and it is ...          1   test\n",
       "4      Bill Paxton has taken the true story of the 19...          1   test\n",
       "...                                                  ...        ...    ...\n",
       "49995  Towards the end of the movie, I felt it was to...          0  train\n",
       "49996  This is the kind of movie that my enemies cont...          0  train\n",
       "49997  I saw 'Descent' last night at the Stockholm Fi...          0  train\n",
       "49998  Some films that you pick up for a pound turn o...          0  train\n",
       "49999  This is one of the dumbest films, I've ever se...          0  train\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So what is a text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Actor turned director Bill Paxton follows up his promising debut, the Gothic-horror \"Frailty\", with this family friendly sports drama about the 1913 U.S. Open where a young American caddy rises from his humble background to play against his Bristish idol in what was dubbed as \"The Greatest Game Ever Played.\" I\\'m no fan of golf, and these scrappy underdog sports flicks are a dime a dozen (most recently done to grand effect with \"Miracle\" and \"Cinderella Man\"), but some how this film was enthralling all the same.<br /><br />The film starts with some creative opening credits (imagine a Disneyfied version of the animated opening credits of HBO\\'s \"Carnivale\" and \"Rome\"), but lumbers along slowly for its first by-the-numbers hour. Once the action moves to the U.S. Open things pick up very well. Paxton does a nice job and shows a knack for effective directorial flourishes (I loved the rain-soaked montage of the action on day two of the open) that propel the plot further or add some unexpected psychological depth to the proceedings. There\\'s some compelling character development when the British Harry Vardon is haunted by images of the aristocrats in black suits and top hats who destroyed his family cottage as a child to make way for a golf course. He also does a good job of visually depicting what goes on in the players\\' heads under pressure. Golf, a painfully boring sport, is brought vividly alive here. Credit should also be given the set designers and costume department for creating an engaging period-piece atmosphere of London and Boston at the beginning of the twentieth century.<br /><br />You know how this is going to end not only because it\\'s based on a true story but also because films in this genre follow the same template over and over, but Paxton puts on a better than average show and perhaps indicates more talent behind the camera than he ever had in front of it. Despite the formulaic nature, this is a nice and easy film to root for that deserves to find an audience.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df['review'][1]\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can also call our text a **document**\n",
    "    - The document determines at which level we will analyse the text. For example, the text above can be analysed in different ways:\n",
    "        - split each sentence to analyse them separately: *each sentence* is then defined as a document\n",
    "        - analyse the whole text: *the whole text* is then defined as a document\n",
    "        - analyse all the reviews that the author has written: *all reviews combined* are then defined as a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which one is the right definition of the document?\n",
    "    - It depends on the task you would like to solve\n",
    "        - Are there any dependencies across the author's reviews? Then it might be a good idea to combine them all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does a document consist of?\n",
    "\n",
    "- WORDS!\n",
    "- In the raw text, words are not structured in any way\n",
    "    - We need structured data to analyse it!\n",
    "    - --> Structure the words in a Bag of Words model (more about that later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. A 'text as data' recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## A. Specify what is your document\n",
    "\n",
    "- Is it every single tweet?\n",
    "- Daily tweets?\n",
    "- Monthly tweets?\n",
    "- Or all tweets a person has ever made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## B. Preprocess the text: Reduce the number of language elements\n",
    "\n",
    "- Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "- Tokenization and stop-words\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C. Apply: What question would you like to answer and what is the right tool?\n",
    "\n",
    "- Machine learning model for sentiment analysis\n",
    "- Lexicons\n",
    "- Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video 14.1: Preprocessing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Preprocessing text data (second step in our recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Different steps in preprocessing:\n",
    "\n",
    "1. Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "2. Tokenization and stop-words\n",
    "3. Stemming and lemmatization\n",
    "\n",
    "Which preprocessing steps that are important depends on the problem you will solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Clean text: ignore casing, HTML markup, non-words\n",
    "\n",
    "- Casing: \n",
    "    - We want \"Movie\" and \"movie\" to be the same word, so we change all letters to lower case\n",
    "- HTML markup:\n",
    "    - In our review example we see there is some unwanted HTML markup left. We want to drop it\n",
    "- Non-words: \n",
    "    - Any other character than words or numbers (non-alphanumeric characters) are typically not important for text data analysis, so we may drop them\n",
    "    - Exceptions:\n",
    "        - Emoticons may very much give information about sentiment in a text\n",
    "        - Dollar signs to indicate a price. Punctuation to indicate decimals in the price\n",
    "    - It all depends on the problem you want to solve!\n",
    "    - Careful: You might not want to remove any non-alphanumeric characters before you tokenize (next step)\n",
    "- Other stuff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Change to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actor turned director bill paxton follows up his promising debut, the gothic-horror \"frailty\", with this family friendly sports drama about the 1913 u.s. open where a young american caddy rises from his humble background to play against his bristish idol in what was dubbed as \"the greatest game ever played.\" i\\'m no fan of golf, and these scrappy underdog sports flicks are a dime a dozen (most recently done to grand effect with \"miracle\" and \"cinderella man\"), but some how this film was enthralling all the same.<br /><br />the film starts with some creative opening credits (imagine a disneyfied version of the animated opening credits of hbo\\'s \"carnivale\" and \"rome\"), but lumbers along slowly for its first by-the-numbers hour. once the action moves to the u.s. open things pick up very well. paxton does a nice job and shows a knack for effective directorial flourishes (i loved the rain-soaked montage of the action on day two of the open) that propel the plot further or add some unexpected psychological depth to the proceedings. there\\'s some compelling character development when the british harry vardon is haunted by images of the aristocrats in black suits and top hats who destroyed his family cottage as a child to make way for a golf course. he also does a good job of visually depicting what goes on in the players\\' heads under pressure. golf, a painfully boring sport, is brought vividly alive here. credit should also be given the set designers and costume department for creating an engaging period-piece atmosphere of london and boston at the beginning of the twentieth century.<br /><br />you know how this is going to end not only because it\\'s based on a true story but also because films in this genre follow the same template over and over, but paxton puts on a better than average show and perhaps indicates more talent behind the camera than he ever had in front of it. despite the formulaic nature, this is a nice and easy film to root for that deserves to find an audience.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_low = review.lower()\n",
    "review_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Remove HTML markup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actor turned director bill paxton follows up his promising debut, the gothic-horror \"frailty\", with this family friendly sports drama about the 1913 u.s. open where a young american caddy rises from his humble background to play against his bristish idol in what was dubbed as \"the greatest game ever played.\" i\\'m no fan of golf, and these scrappy underdog sports flicks are a dime a dozen (most recently done to grand effect with \"miracle\" and \"cinderella man\"), but some how this film was enthralling all the same.  the film starts with some creative opening credits (imagine a disneyfied version of the animated opening credits of hbo\\'s \"carnivale\" and \"rome\"), but lumbers along slowly for its first by-the-numbers hour. once the action moves to the u.s. open things pick up very well. paxton does a nice job and shows a knack for effective directorial flourishes (i loved the rain-soaked montage of the action on day two of the open) that propel the plot further or add some unexpected psychological depth to the proceedings. there\\'s some compelling character development when the british harry vardon is haunted by images of the aristocrats in black suits and top hats who destroyed his family cottage as a child to make way for a golf course. he also does a good job of visually depicting what goes on in the players\\' heads under pressure. golf, a painfully boring sport, is brought vividly alive here. credit should also be given the set designers and costume department for creating an engaging period-piece atmosphere of london and boston at the beginning of the twentieth century.  you know how this is going to end not only because it\\'s based on a true story but also because films in this genre follow the same template over and over, but paxton puts on a better than average show and perhaps indicates more talent behind the camera than he ever had in front of it. despite the formulaic nature, this is a nice and easy film to root for that deserves to find an audience.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "review_noHTML = re.sub(r'<[^>]*>', ' ', review_low) #Regex pattern matches the HTML markup surrounded by \"<\" and \">\" and replace it with ' ' using the method sub()\n",
    "review_noHTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Remove all characters that are not words or numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actor turned director bill paxton follows up his promising debut the gothichorror frailty with this family friendly sports drama about the 1913 us open where a young american caddy rises from his humble background to play against his bristish idol in what was dubbed as the greatest game ever played im no fan of golf and these scrappy underdog sports flicks are a dime a dozen most recently done to grand effect with miracle and cinderella man but some how this film was enthralling all the same  the film starts with some creative opening credits imagine a disneyfied version of the animated opening credits of hbos carnivale and rome but lumbers along slowly for its first bythenumbers hour once the action moves to the us open things pick up very well paxton does a nice job and shows a knack for effective directorial flourishes i loved the rainsoaked montage of the action on day two of the open that propel the plot further or add some unexpected psychological depth to the proceedings theres some compelling character development when the british harry vardon is haunted by images of the aristocrats in black suits and top hats who destroyed his family cottage as a child to make way for a golf course he also does a good job of visually depicting what goes on in the players heads under pressure golf a painfully boring sport is brought vividly alive here credit should also be given the set designers and costume department for creating an engaging periodpiece atmosphere of london and boston at the beginning of the twentieth century  you know how this is going to end not only because its based on a true story but also because films in this genre follow the same template over and over but paxton puts on a better than average show and perhaps indicates more talent behind the camera than he ever had in front of it despite the formulaic nature this is a nice and easy film to root for that deserves to find an audience'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_cleaned = re.sub(r'[^\\w\\s]','',review_noHTML) #Regex pattern matches any non-alphanumeric characters and replace them with '' using the method sub()\n",
    "review_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Other stuff?\n",
    "\n",
    "- There may be other things you need to remove before you are ready to move on\n",
    "- It depends on the texts you are dealing with and the problem you want to solve\n",
    "    - Investigate the texts\n",
    "    - Make sure that you keep all the important stuff and remove the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now apply our cleaning process on all reviews in the dataset to work with it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cleaner(document):\n",
    "    document = document.lower() #To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document) #Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s]','', document) #Remove non-alphanumeric characters\n",
    "    return document\n",
    "\n",
    "df['review'] = df['review'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i went and saw this movie last night after bei...\n",
       "1        actor turned director bill paxton follows up h...\n",
       "2        as a recreational golfer with some knowledge o...\n",
       "3        i saw this film in a sneak preview and it is d...\n",
       "4        bill paxton has taken the true story of the 19...\n",
       "                               ...                        \n",
       "49995    towards the end of the movie i felt it was too...\n",
       "49996    this is the kind of movie that my enemies cont...\n",
       "49997    i saw descent last night at the stockholm film...\n",
       "49998    some films that you pick up for a pound turn o...\n",
       "49999    this is one of the dumbest films ive ever seen...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Tokenization (I/II)\n",
    "\n",
    "- Tokenization is about splitting the document into meaningful elements (/*tokens*)\n",
    "    - Tokens can be thought of as words in a sentence or sentences in a text\n",
    "- Simplest tokenization: Split the cleaned document at its whitespaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'turned',\n",
       " 'director',\n",
       " 'bill',\n",
       " 'paxton',\n",
       " 'follows',\n",
       " 'up',\n",
       " 'his',\n",
       " 'promising',\n",
       " 'debut',\n",
       " 'the',\n",
       " 'gothichorror',\n",
       " 'frailty',\n",
       " 'with',\n",
       " 'this',\n",
       " 'family',\n",
       " 'friendly',\n",
       " 'sports',\n",
       " 'drama',\n",
       " 'about',\n",
       " 'the',\n",
       " '1913',\n",
       " 'us',\n",
       " 'open',\n",
       " 'where',\n",
       " 'a',\n",
       " 'young',\n",
       " 'american',\n",
       " 'caddy',\n",
       " 'rises',\n",
       " 'from',\n",
       " 'his',\n",
       " 'humble',\n",
       " 'background',\n",
       " 'to',\n",
       " 'play',\n",
       " 'against',\n",
       " 'his',\n",
       " 'bristish',\n",
       " 'idol',\n",
       " 'in',\n",
       " 'what',\n",
       " 'was',\n",
       " 'dubbed',\n",
       " 'as',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'game',\n",
       " 'ever',\n",
       " 'played',\n",
       " 'im',\n",
       " 'no',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'golf',\n",
       " 'and',\n",
       " 'these',\n",
       " 'scrappy',\n",
       " 'underdog',\n",
       " 'sports',\n",
       " 'flicks',\n",
       " 'are',\n",
       " 'a',\n",
       " 'dime',\n",
       " 'a',\n",
       " 'dozen',\n",
       " 'most',\n",
       " 'recently',\n",
       " 'done',\n",
       " 'to',\n",
       " 'grand',\n",
       " 'effect',\n",
       " 'with',\n",
       " 'miracle',\n",
       " 'and',\n",
       " 'cinderella',\n",
       " 'man',\n",
       " 'but',\n",
       " 'some',\n",
       " 'how',\n",
       " 'this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'enthralling',\n",
       " 'all',\n",
       " 'the',\n",
       " 'same',\n",
       " 'the',\n",
       " 'film',\n",
       " 'starts',\n",
       " 'with',\n",
       " 'some',\n",
       " 'creative',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'imagine',\n",
       " 'a',\n",
       " 'disneyfied',\n",
       " 'version',\n",
       " 'of',\n",
       " 'the',\n",
       " 'animated',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'of',\n",
       " 'hbos',\n",
       " 'carnivale',\n",
       " 'and',\n",
       " 'rome',\n",
       " 'but',\n",
       " 'lumbers',\n",
       " 'along',\n",
       " 'slowly',\n",
       " 'for',\n",
       " 'its',\n",
       " 'first',\n",
       " 'bythenumbers',\n",
       " 'hour',\n",
       " 'once',\n",
       " 'the',\n",
       " 'action',\n",
       " 'moves',\n",
       " 'to',\n",
       " 'the',\n",
       " 'us',\n",
       " 'open',\n",
       " 'things',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'very',\n",
       " 'well',\n",
       " 'paxton',\n",
       " 'does',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'job',\n",
       " 'and',\n",
       " 'shows',\n",
       " 'a',\n",
       " 'knack',\n",
       " 'for',\n",
       " 'effective',\n",
       " 'directorial',\n",
       " 'flourishes',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'rainsoaked',\n",
       " 'montage',\n",
       " 'of',\n",
       " 'the',\n",
       " 'action',\n",
       " 'on',\n",
       " 'day',\n",
       " 'two',\n",
       " 'of',\n",
       " 'the',\n",
       " 'open',\n",
       " 'that',\n",
       " 'propel',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'further',\n",
       " 'or',\n",
       " 'add',\n",
       " 'some',\n",
       " 'unexpected',\n",
       " 'psychological',\n",
       " 'depth',\n",
       " 'to',\n",
       " 'the',\n",
       " 'proceedings',\n",
       " 'theres',\n",
       " 'some',\n",
       " 'compelling',\n",
       " 'character',\n",
       " 'development',\n",
       " 'when',\n",
       " 'the',\n",
       " 'british',\n",
       " 'harry',\n",
       " 'vardon',\n",
       " 'is',\n",
       " 'haunted',\n",
       " 'by',\n",
       " 'images',\n",
       " 'of',\n",
       " 'the',\n",
       " 'aristocrats',\n",
       " 'in',\n",
       " 'black',\n",
       " 'suits',\n",
       " 'and',\n",
       " 'top',\n",
       " 'hats',\n",
       " 'who',\n",
       " 'destroyed',\n",
       " 'his',\n",
       " 'family',\n",
       " 'cottage',\n",
       " 'as',\n",
       " 'a',\n",
       " 'child',\n",
       " 'to',\n",
       " 'make',\n",
       " 'way',\n",
       " 'for',\n",
       " 'a',\n",
       " 'golf',\n",
       " 'course',\n",
       " 'he',\n",
       " 'also',\n",
       " 'does',\n",
       " 'a',\n",
       " 'good',\n",
       " 'job',\n",
       " 'of',\n",
       " 'visually',\n",
       " 'depicting',\n",
       " 'what',\n",
       " 'goes',\n",
       " 'on',\n",
       " 'in',\n",
       " 'the',\n",
       " 'players',\n",
       " 'heads',\n",
       " 'under',\n",
       " 'pressure',\n",
       " 'golf',\n",
       " 'a',\n",
       " 'painfully',\n",
       " 'boring',\n",
       " 'sport',\n",
       " 'is',\n",
       " 'brought',\n",
       " 'vividly',\n",
       " 'alive',\n",
       " 'here',\n",
       " 'credit',\n",
       " 'should',\n",
       " 'also',\n",
       " 'be',\n",
       " 'given',\n",
       " 'the',\n",
       " 'set',\n",
       " 'designers',\n",
       " 'and',\n",
       " 'costume',\n",
       " 'department',\n",
       " 'for',\n",
       " 'creating',\n",
       " 'an',\n",
       " 'engaging',\n",
       " 'periodpiece',\n",
       " 'atmosphere',\n",
       " 'of',\n",
       " 'london',\n",
       " 'and',\n",
       " 'boston',\n",
       " 'at',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'of',\n",
       " 'the',\n",
       " 'twentieth',\n",
       " 'century',\n",
       " 'you',\n",
       " 'know',\n",
       " 'how',\n",
       " 'this',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'end',\n",
       " 'not',\n",
       " 'only',\n",
       " 'because',\n",
       " 'its',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " 'true',\n",
       " 'story',\n",
       " 'but',\n",
       " 'also',\n",
       " 'because',\n",
       " 'films',\n",
       " 'in',\n",
       " 'this',\n",
       " 'genre',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'same',\n",
       " 'template',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'but',\n",
       " 'paxton',\n",
       " 'puts',\n",
       " 'on',\n",
       " 'a',\n",
       " 'better',\n",
       " 'than',\n",
       " 'average',\n",
       " 'show',\n",
       " 'and',\n",
       " 'perhaps',\n",
       " 'indicates',\n",
       " 'more',\n",
       " 'talent',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'camera',\n",
       " 'than',\n",
       " 'he',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'in',\n",
       " 'front',\n",
       " 'of',\n",
       " 'it',\n",
       " 'despite',\n",
       " 'the',\n",
       " 'formulaic',\n",
       " 'nature',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'easy',\n",
       " 'film',\n",
       " 'to',\n",
       " 'root',\n",
       " 'for',\n",
       " 'that',\n",
       " 'deserves',\n",
       " 'to',\n",
       " 'find',\n",
       " 'an',\n",
       " 'audience']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split at whitespace with the split() method\n",
    "review_tokens = review_cleaned.split()\n",
    "review_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Tokenization (II/II)\n",
    "\n",
    "- The simple tokenization might not suffice in some cases:\n",
    "    - How should we treat abbreviations like Ph.D.? And dollar signs before a price? And punctuation that indicates decimals?\n",
    "- The NLTK library has some [tokenizer packages](https://www.nltk.org/api/nltk.tokenize.html) that can hep you:\n",
    "    - `word_tokenize()` splits the words \n",
    "    - If you have twitter data, then `TweetTokenizer()` will keep the hashtag intact\n",
    "    - You can also define your own tokenization pattern using regex with `regexp_tokenize()`\n",
    "- But in many cases it is just fine to use `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'turned',\n",
       " 'director',\n",
       " 'bill',\n",
       " 'paxton',\n",
       " 'follows',\n",
       " 'up',\n",
       " 'his',\n",
       " 'promising',\n",
       " 'debut',\n",
       " 'the',\n",
       " 'gothichorror',\n",
       " 'frailty',\n",
       " 'with',\n",
       " 'this',\n",
       " 'family',\n",
       " 'friendly',\n",
       " 'sports',\n",
       " 'drama',\n",
       " 'about',\n",
       " 'the',\n",
       " '1913',\n",
       " 'us',\n",
       " 'open',\n",
       " 'where',\n",
       " 'a',\n",
       " 'young',\n",
       " 'american',\n",
       " 'caddy',\n",
       " 'rises',\n",
       " 'from',\n",
       " 'his',\n",
       " 'humble',\n",
       " 'background',\n",
       " 'to',\n",
       " 'play',\n",
       " 'against',\n",
       " 'his',\n",
       " 'bristish',\n",
       " 'idol',\n",
       " 'in',\n",
       " 'what',\n",
       " 'was',\n",
       " 'dubbed',\n",
       " 'as',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'game',\n",
       " 'ever',\n",
       " 'played',\n",
       " 'im',\n",
       " 'no',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'golf',\n",
       " 'and',\n",
       " 'these',\n",
       " 'scrappy',\n",
       " 'underdog',\n",
       " 'sports',\n",
       " 'flicks',\n",
       " 'are',\n",
       " 'a',\n",
       " 'dime',\n",
       " 'a',\n",
       " 'dozen',\n",
       " 'most',\n",
       " 'recently',\n",
       " 'done',\n",
       " 'to',\n",
       " 'grand',\n",
       " 'effect',\n",
       " 'with',\n",
       " 'miracle',\n",
       " 'and',\n",
       " 'cinderella',\n",
       " 'man',\n",
       " 'but',\n",
       " 'some',\n",
       " 'how',\n",
       " 'this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'enthralling',\n",
       " 'all',\n",
       " 'the',\n",
       " 'same',\n",
       " 'the',\n",
       " 'film',\n",
       " 'starts',\n",
       " 'with',\n",
       " 'some',\n",
       " 'creative',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'imagine',\n",
       " 'a',\n",
       " 'disneyfied',\n",
       " 'version',\n",
       " 'of',\n",
       " 'the',\n",
       " 'animated',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'of',\n",
       " 'hbos',\n",
       " 'carnivale',\n",
       " 'and',\n",
       " 'rome',\n",
       " 'but',\n",
       " 'lumbers',\n",
       " 'along',\n",
       " 'slowly',\n",
       " 'for',\n",
       " 'its',\n",
       " 'first',\n",
       " 'bythenumbers',\n",
       " 'hour',\n",
       " 'once',\n",
       " 'the',\n",
       " 'action',\n",
       " 'moves',\n",
       " 'to',\n",
       " 'the',\n",
       " 'us',\n",
       " 'open',\n",
       " 'things',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'very',\n",
       " 'well',\n",
       " 'paxton',\n",
       " 'does',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'job',\n",
       " 'and',\n",
       " 'shows',\n",
       " 'a',\n",
       " 'knack',\n",
       " 'for',\n",
       " 'effective',\n",
       " 'directorial',\n",
       " 'flourishes',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'rainsoaked',\n",
       " 'montage',\n",
       " 'of',\n",
       " 'the',\n",
       " 'action',\n",
       " 'on',\n",
       " 'day',\n",
       " 'two',\n",
       " 'of',\n",
       " 'the',\n",
       " 'open',\n",
       " 'that',\n",
       " 'propel',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'further',\n",
       " 'or',\n",
       " 'add',\n",
       " 'some',\n",
       " 'unexpected',\n",
       " 'psychological',\n",
       " 'depth',\n",
       " 'to',\n",
       " 'the',\n",
       " 'proceedings',\n",
       " 'theres',\n",
       " 'some',\n",
       " 'compelling',\n",
       " 'character',\n",
       " 'development',\n",
       " 'when',\n",
       " 'the',\n",
       " 'british',\n",
       " 'harry',\n",
       " 'vardon',\n",
       " 'is',\n",
       " 'haunted',\n",
       " 'by',\n",
       " 'images',\n",
       " 'of',\n",
       " 'the',\n",
       " 'aristocrats',\n",
       " 'in',\n",
       " 'black',\n",
       " 'suits',\n",
       " 'and',\n",
       " 'top',\n",
       " 'hats',\n",
       " 'who',\n",
       " 'destroyed',\n",
       " 'his',\n",
       " 'family',\n",
       " 'cottage',\n",
       " 'as',\n",
       " 'a',\n",
       " 'child',\n",
       " 'to',\n",
       " 'make',\n",
       " 'way',\n",
       " 'for',\n",
       " 'a',\n",
       " 'golf',\n",
       " 'course',\n",
       " 'he',\n",
       " 'also',\n",
       " 'does',\n",
       " 'a',\n",
       " 'good',\n",
       " 'job',\n",
       " 'of',\n",
       " 'visually',\n",
       " 'depicting',\n",
       " 'what',\n",
       " 'goes',\n",
       " 'on',\n",
       " 'in',\n",
       " 'the',\n",
       " 'players',\n",
       " 'heads',\n",
       " 'under',\n",
       " 'pressure',\n",
       " 'golf',\n",
       " 'a',\n",
       " 'painfully',\n",
       " 'boring',\n",
       " 'sport',\n",
       " 'is',\n",
       " 'brought',\n",
       " 'vividly',\n",
       " 'alive',\n",
       " 'here',\n",
       " 'credit',\n",
       " 'should',\n",
       " 'also',\n",
       " 'be',\n",
       " 'given',\n",
       " 'the',\n",
       " 'set',\n",
       " 'designers',\n",
       " 'and',\n",
       " 'costume',\n",
       " 'department',\n",
       " 'for',\n",
       " 'creating',\n",
       " 'an',\n",
       " 'engaging',\n",
       " 'periodpiece',\n",
       " 'atmosphere',\n",
       " 'of',\n",
       " 'london',\n",
       " 'and',\n",
       " 'boston',\n",
       " 'at',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'of',\n",
       " 'the',\n",
       " 'twentieth',\n",
       " 'century',\n",
       " 'you',\n",
       " 'know',\n",
       " 'how',\n",
       " 'this',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'end',\n",
       " 'not',\n",
       " 'only',\n",
       " 'because',\n",
       " 'its',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " 'true',\n",
       " 'story',\n",
       " 'but',\n",
       " 'also',\n",
       " 'because',\n",
       " 'films',\n",
       " 'in',\n",
       " 'this',\n",
       " 'genre',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'same',\n",
       " 'template',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'but',\n",
       " 'paxton',\n",
       " 'puts',\n",
       " 'on',\n",
       " 'a',\n",
       " 'better',\n",
       " 'than',\n",
       " 'average',\n",
       " 'show',\n",
       " 'and',\n",
       " 'perhaps',\n",
       " 'indicates',\n",
       " 'more',\n",
       " 'talent',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'camera',\n",
       " 'than',\n",
       " 'he',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'in',\n",
       " 'front',\n",
       " 'of',\n",
       " 'it',\n",
       " 'despite',\n",
       " 'the',\n",
       " 'formulaic',\n",
       " 'nature',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'easy',\n",
       " 'film',\n",
       " 'to',\n",
       " 'root',\n",
       " 'for',\n",
       " 'that',\n",
       " 'deserves',\n",
       " 'to',\n",
       " 'find',\n",
       " 'an',\n",
       " 'audience']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "review_tokens = nltk.tokenize.word_tokenize(review_cleaned)\n",
    "review_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stop-words:\n",
    "\n",
    "- Words that are extremely common in all texts\n",
    "- Probably bear no useful information about the text --> we want to remove them\n",
    "- Examples: *is, and, has, like...*\n",
    "\n",
    "Use the NLTK library of 127 English stop-words\n",
    "- NLTK (Natural Language ToolKit) is a popular Python package for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/fch/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'turned',\n",
       " 'director',\n",
       " 'bill',\n",
       " 'paxton',\n",
       " 'follows',\n",
       " 'promising',\n",
       " 'debut',\n",
       " 'gothichorror',\n",
       " 'frailty',\n",
       " 'family',\n",
       " 'friendly',\n",
       " 'sports',\n",
       " 'drama',\n",
       " '1913',\n",
       " 'us',\n",
       " 'open',\n",
       " 'young',\n",
       " 'american',\n",
       " 'caddy',\n",
       " 'rises',\n",
       " 'humble',\n",
       " 'background',\n",
       " 'play',\n",
       " 'bristish',\n",
       " 'idol',\n",
       " 'dubbed',\n",
       " 'greatest',\n",
       " 'game',\n",
       " 'ever',\n",
       " 'played',\n",
       " 'im',\n",
       " 'fan',\n",
       " 'golf',\n",
       " 'scrappy',\n",
       " 'underdog',\n",
       " 'sports',\n",
       " 'flicks',\n",
       " 'dime',\n",
       " 'dozen',\n",
       " 'recently',\n",
       " 'done',\n",
       " 'grand',\n",
       " 'effect',\n",
       " 'miracle',\n",
       " 'cinderella',\n",
       " 'man',\n",
       " 'film',\n",
       " 'enthralling',\n",
       " 'film',\n",
       " 'starts',\n",
       " 'creative',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'imagine',\n",
       " 'disneyfied',\n",
       " 'version',\n",
       " 'animated',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'hbos',\n",
       " 'carnivale',\n",
       " 'rome',\n",
       " 'lumbers',\n",
       " 'along',\n",
       " 'slowly',\n",
       " 'first',\n",
       " 'bythenumbers',\n",
       " 'hour',\n",
       " 'action',\n",
       " 'moves',\n",
       " 'us',\n",
       " 'open',\n",
       " 'things',\n",
       " 'pick',\n",
       " 'well',\n",
       " 'paxton',\n",
       " 'nice',\n",
       " 'job',\n",
       " 'shows',\n",
       " 'knack',\n",
       " 'effective',\n",
       " 'directorial',\n",
       " 'flourishes',\n",
       " 'loved',\n",
       " 'rainsoaked',\n",
       " 'montage',\n",
       " 'action',\n",
       " 'day',\n",
       " 'two',\n",
       " 'open',\n",
       " 'propel',\n",
       " 'plot',\n",
       " 'add',\n",
       " 'unexpected',\n",
       " 'psychological',\n",
       " 'depth',\n",
       " 'proceedings',\n",
       " 'theres',\n",
       " 'compelling',\n",
       " 'character',\n",
       " 'development',\n",
       " 'british',\n",
       " 'harry',\n",
       " 'vardon',\n",
       " 'haunted',\n",
       " 'images',\n",
       " 'aristocrats',\n",
       " 'black',\n",
       " 'suits',\n",
       " 'top',\n",
       " 'hats',\n",
       " 'destroyed',\n",
       " 'family',\n",
       " 'cottage',\n",
       " 'child',\n",
       " 'make',\n",
       " 'way',\n",
       " 'golf',\n",
       " 'course',\n",
       " 'also',\n",
       " 'good',\n",
       " 'job',\n",
       " 'visually',\n",
       " 'depicting',\n",
       " 'goes',\n",
       " 'players',\n",
       " 'heads',\n",
       " 'pressure',\n",
       " 'golf',\n",
       " 'painfully',\n",
       " 'boring',\n",
       " 'sport',\n",
       " 'brought',\n",
       " 'vividly',\n",
       " 'alive',\n",
       " 'credit',\n",
       " 'also',\n",
       " 'given',\n",
       " 'set',\n",
       " 'designers',\n",
       " 'costume',\n",
       " 'department',\n",
       " 'creating',\n",
       " 'engaging',\n",
       " 'periodpiece',\n",
       " 'atmosphere',\n",
       " 'london',\n",
       " 'boston',\n",
       " 'beginning',\n",
       " 'twentieth',\n",
       " 'century',\n",
       " 'know',\n",
       " 'going',\n",
       " 'end',\n",
       " 'based',\n",
       " 'true',\n",
       " 'story',\n",
       " 'also',\n",
       " 'films',\n",
       " 'genre',\n",
       " 'follow',\n",
       " 'template',\n",
       " 'paxton',\n",
       " 'puts',\n",
       " 'better',\n",
       " 'average',\n",
       " 'show',\n",
       " 'perhaps',\n",
       " 'indicates',\n",
       " 'talent',\n",
       " 'behind',\n",
       " 'camera',\n",
       " 'ever',\n",
       " 'front',\n",
       " 'despite',\n",
       " 'formulaic',\n",
       " 'nature',\n",
       " 'nice',\n",
       " 'easy',\n",
       " 'film',\n",
       " 'root',\n",
       " 'deserves',\n",
       " 'find',\n",
       " 'audience']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "review_nostop = [i for i in review_tokens if i not in stop]\n",
    "review_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "print(len(review_tokens))\n",
    "print(len(review_nostop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Stemming and lemmatization\n",
    "\n",
    "#### Stemming:\n",
    "- The process of transforming a word into its root form\n",
    "- Allows us to map related words to the same stem\n",
    "- Examples: `'runners', 'run', 'running'` becomes `'runner', 'run', 'run'`. `'wonderful'` becomes `'wonder'`.\n",
    "- You can use the Porter stemmer in the NLTK library to stem your words: `PorterStemmer()`\n",
    "    - With stemming we generally just remove the suffix of the word: very simple method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'turn',\n",
       " 'director',\n",
       " 'bill',\n",
       " 'paxton',\n",
       " 'follow',\n",
       " 'promis',\n",
       " 'debut',\n",
       " 'gothichorror',\n",
       " 'frailti',\n",
       " 'famili',\n",
       " 'friendli',\n",
       " 'sport',\n",
       " 'drama',\n",
       " '1913',\n",
       " 'us',\n",
       " 'open',\n",
       " 'young',\n",
       " 'american',\n",
       " 'caddi',\n",
       " 'rise',\n",
       " 'humbl',\n",
       " 'background',\n",
       " 'play',\n",
       " 'bristish',\n",
       " 'idol',\n",
       " 'dub',\n",
       " 'greatest',\n",
       " 'game',\n",
       " 'ever',\n",
       " 'play',\n",
       " 'im',\n",
       " 'fan',\n",
       " 'golf',\n",
       " 'scrappi',\n",
       " 'underdog',\n",
       " 'sport',\n",
       " 'flick',\n",
       " 'dime',\n",
       " 'dozen',\n",
       " 'recent',\n",
       " 'done',\n",
       " 'grand',\n",
       " 'effect',\n",
       " 'miracl',\n",
       " 'cinderella',\n",
       " 'man',\n",
       " 'film',\n",
       " 'enthral',\n",
       " 'film',\n",
       " 'start',\n",
       " 'creativ',\n",
       " 'open',\n",
       " 'credit',\n",
       " 'imagin',\n",
       " 'disneyfi',\n",
       " 'version',\n",
       " 'anim',\n",
       " 'open',\n",
       " 'credit',\n",
       " 'hbo',\n",
       " 'carnival',\n",
       " 'rome',\n",
       " 'lumber',\n",
       " 'along',\n",
       " 'slowli',\n",
       " 'first',\n",
       " 'bythenumb',\n",
       " 'hour',\n",
       " 'action',\n",
       " 'move',\n",
       " 'us',\n",
       " 'open',\n",
       " 'thing',\n",
       " 'pick',\n",
       " 'well',\n",
       " 'paxton',\n",
       " 'nice',\n",
       " 'job',\n",
       " 'show',\n",
       " 'knack',\n",
       " 'effect',\n",
       " 'directori',\n",
       " 'flourish',\n",
       " 'love',\n",
       " 'rainsoak',\n",
       " 'montag',\n",
       " 'action',\n",
       " 'day',\n",
       " 'two',\n",
       " 'open',\n",
       " 'propel',\n",
       " 'plot',\n",
       " 'add',\n",
       " 'unexpect',\n",
       " 'psycholog',\n",
       " 'depth',\n",
       " 'proceed',\n",
       " 'there',\n",
       " 'compel',\n",
       " 'charact',\n",
       " 'develop',\n",
       " 'british',\n",
       " 'harri',\n",
       " 'vardon',\n",
       " 'haunt',\n",
       " 'imag',\n",
       " 'aristocrat',\n",
       " 'black',\n",
       " 'suit',\n",
       " 'top',\n",
       " 'hat',\n",
       " 'destroy',\n",
       " 'famili',\n",
       " 'cottag',\n",
       " 'child',\n",
       " 'make',\n",
       " 'way',\n",
       " 'golf',\n",
       " 'cours',\n",
       " 'also',\n",
       " 'good',\n",
       " 'job',\n",
       " 'visual',\n",
       " 'depict',\n",
       " 'goe',\n",
       " 'player',\n",
       " 'head',\n",
       " 'pressur',\n",
       " 'golf',\n",
       " 'pain',\n",
       " 'bore',\n",
       " 'sport',\n",
       " 'brought',\n",
       " 'vividli',\n",
       " 'aliv',\n",
       " 'credit',\n",
       " 'also',\n",
       " 'given',\n",
       " 'set',\n",
       " 'design',\n",
       " 'costum',\n",
       " 'depart',\n",
       " 'creat',\n",
       " 'engag',\n",
       " 'periodpiec',\n",
       " 'atmospher',\n",
       " 'london',\n",
       " 'boston',\n",
       " 'begin',\n",
       " 'twentieth',\n",
       " 'centuri',\n",
       " 'know',\n",
       " 'go',\n",
       " 'end',\n",
       " 'base',\n",
       " 'true',\n",
       " 'stori',\n",
       " 'also',\n",
       " 'film',\n",
       " 'genr',\n",
       " 'follow',\n",
       " 'templat',\n",
       " 'paxton',\n",
       " 'put',\n",
       " 'better',\n",
       " 'averag',\n",
       " 'show',\n",
       " 'perhap',\n",
       " 'indic',\n",
       " 'talent',\n",
       " 'behind',\n",
       " 'camera',\n",
       " 'ever',\n",
       " 'front',\n",
       " 'despit',\n",
       " 'formula',\n",
       " 'natur',\n",
       " 'nice',\n",
       " 'easi',\n",
       " 'film',\n",
       " 'root',\n",
       " 'deserv',\n",
       " 'find',\n",
       " 'audienc']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the words\n",
    "porter = nltk.PorterStemmer()\n",
    "review_stemmed = [porter.stem(i) for i in review_nostop]\n",
    "review_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lemmatization:\n",
    "\n",
    "- Stemming can create non-real words in some cases (see above)\n",
    "- Lemmatization is more advanced and seeks to find the grammatically correct form of the word (the lemma)\n",
    "    - Example: `'coding', 'code', 'coded'` will all be lemmatized to `'code'`\n",
    "- Lemmatization demands a lot of computer power --> it is slow\n",
    "- In practice there are little difference between stemming and lemmatization on the performance of text classification\n",
    "    - [Influence of Word Normalization on Text Classification](https://www.researchgate.net/publication/250030718_Influence_of_Word_Normalization_on_Text_Classification)\n",
    "\n",
    "You can use the [WordNet](https://wordnet.princeton.edu/) lemmatizer from NLTK\n",
    "- WordNet is a large lexical database of English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/fch/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/fch/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'turned',\n",
       " 'director',\n",
       " 'bill',\n",
       " 'paxton',\n",
       " 'follows',\n",
       " 'promising',\n",
       " 'debut',\n",
       " 'gothichorror',\n",
       " 'frailty',\n",
       " 'family',\n",
       " 'friendly',\n",
       " 'sport',\n",
       " 'drama',\n",
       " '1913',\n",
       " 'u',\n",
       " 'open',\n",
       " 'young',\n",
       " 'american',\n",
       " 'caddy',\n",
       " 'rise',\n",
       " 'humble',\n",
       " 'background',\n",
       " 'play',\n",
       " 'bristish',\n",
       " 'idol',\n",
       " 'dubbed',\n",
       " 'greatest',\n",
       " 'game',\n",
       " 'ever',\n",
       " 'played',\n",
       " 'im',\n",
       " 'fan',\n",
       " 'golf',\n",
       " 'scrappy',\n",
       " 'underdog',\n",
       " 'sport',\n",
       " 'flick',\n",
       " 'dime',\n",
       " 'dozen',\n",
       " 'recently',\n",
       " 'done',\n",
       " 'grand',\n",
       " 'effect',\n",
       " 'miracle',\n",
       " 'cinderella',\n",
       " 'man',\n",
       " 'film',\n",
       " 'enthralling',\n",
       " 'film',\n",
       " 'start',\n",
       " 'creative',\n",
       " 'opening',\n",
       " 'credit',\n",
       " 'imagine',\n",
       " 'disneyfied',\n",
       " 'version',\n",
       " 'animated',\n",
       " 'opening',\n",
       " 'credit',\n",
       " 'hbos',\n",
       " 'carnivale',\n",
       " 'rome',\n",
       " 'lumber',\n",
       " 'along',\n",
       " 'slowly',\n",
       " 'first',\n",
       " 'bythenumbers',\n",
       " 'hour',\n",
       " 'action',\n",
       " 'move',\n",
       " 'u',\n",
       " 'open',\n",
       " 'thing',\n",
       " 'pick',\n",
       " 'well',\n",
       " 'paxton',\n",
       " 'nice',\n",
       " 'job',\n",
       " 'show',\n",
       " 'knack',\n",
       " 'effective',\n",
       " 'directorial',\n",
       " 'flourish',\n",
       " 'loved',\n",
       " 'rainsoaked',\n",
       " 'montage',\n",
       " 'action',\n",
       " 'day',\n",
       " 'two',\n",
       " 'open',\n",
       " 'propel',\n",
       " 'plot',\n",
       " 'add',\n",
       " 'unexpected',\n",
       " 'psychological',\n",
       " 'depth',\n",
       " 'proceeding',\n",
       " 'there',\n",
       " 'compelling',\n",
       " 'character',\n",
       " 'development',\n",
       " 'british',\n",
       " 'harry',\n",
       " 'vardon',\n",
       " 'haunted',\n",
       " 'image',\n",
       " 'aristocrat',\n",
       " 'black',\n",
       " 'suit',\n",
       " 'top',\n",
       " 'hat',\n",
       " 'destroyed',\n",
       " 'family',\n",
       " 'cottage',\n",
       " 'child',\n",
       " 'make',\n",
       " 'way',\n",
       " 'golf',\n",
       " 'course',\n",
       " 'also',\n",
       " 'good',\n",
       " 'job',\n",
       " 'visually',\n",
       " 'depicting',\n",
       " 'go',\n",
       " 'player',\n",
       " 'head',\n",
       " 'pressure',\n",
       " 'golf',\n",
       " 'painfully',\n",
       " 'boring',\n",
       " 'sport',\n",
       " 'brought',\n",
       " 'vividly',\n",
       " 'alive',\n",
       " 'credit',\n",
       " 'also',\n",
       " 'given',\n",
       " 'set',\n",
       " 'designer',\n",
       " 'costume',\n",
       " 'department',\n",
       " 'creating',\n",
       " 'engaging',\n",
       " 'periodpiece',\n",
       " 'atmosphere',\n",
       " 'london',\n",
       " 'boston',\n",
       " 'beginning',\n",
       " 'twentieth',\n",
       " 'century',\n",
       " 'know',\n",
       " 'going',\n",
       " 'end',\n",
       " 'based',\n",
       " 'true',\n",
       " 'story',\n",
       " 'also',\n",
       " 'film',\n",
       " 'genre',\n",
       " 'follow',\n",
       " 'template',\n",
       " 'paxton',\n",
       " 'put',\n",
       " 'better',\n",
       " 'average',\n",
       " 'show',\n",
       " 'perhaps',\n",
       " 'indicates',\n",
       " 'talent',\n",
       " 'behind',\n",
       " 'camera',\n",
       " 'ever',\n",
       " 'front',\n",
       " 'despite',\n",
       " 'formulaic',\n",
       " 'nature',\n",
       " 'nice',\n",
       " 'easy',\n",
       " 'film',\n",
       " 'root',\n",
       " 'deserves',\n",
       " 'find',\n",
       " 'audience']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the words with the WordNetLemmatizer\n",
    "nltk.download('omw-1.4') #Download OpenMultilingualWordnet\n",
    "nltk.download('wordnet')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "review_lemma = [wnl.lemmatize(i) for i in review_nostop]\n",
    "review_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video 14.2: The Bag of Words model and tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bag of Words model\n",
    "\n",
    "Read more about the bag of words model in this article: https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "- It can be a good starting point to go into more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actor turned director bill paxton follows up his promising debut the gothichorror frailty with this family friendly sports drama about the 1913 us open where a young american caddy rises from his humble background to play against his bristish idol in what was dubbed as the greatest game ever played im no fan of golf and these scrappy underdog sports flicks are a dime a dozen most recently done to grand effect with miracle and cinderella man but some how this film was enthralling all the same  the film starts with some creative opening credits imagine a disneyfied version of the animated opening credits of hbos carnivale and rome but lumbers along slowly for its first bythenumbers hour once the action moves to the us open things pick up very well paxton does a nice job and shows a knack for effective directorial flourishes i loved the rainsoaked montage of the action on day two of the open that propel the plot further or add some unexpected psychological depth to the proceedings theres some compelling character development when the british harry vardon is haunted by images of the aristocrats in black suits and top hats who destroyed his family cottage as a child to make way for a golf course he also does a good job of visually depicting what goes on in the players heads under pressure golf a painfully boring sport is brought vividly alive here credit should also be given the set designers and costume department for creating an engaging periodpiece atmosphere of london and boston at the beginning of the twentieth century  you know how this is going to end not only because its based on a true story but also because films in this genre follow the same template over and over but paxton puts on a better than average show and perhaps indicates more talent behind the camera than he ever had in front of it despite the formulaic nature this is a nice and easy film to root for that deserves to find an audience'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To exploit the information in text data we need to structure it in some way\n",
    "    - Raw text is not structured\n",
    "- A simple way to structure the documents/texts is the Bag of Words model\n",
    "    - The Bag of Words model simply counts the number of times each word occurs in a document\n",
    "    - That way we can store all documents and word counts in one big matrix (a term-document frequency matrix):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A Bag of Words model:\n",
    "<img src=\"https://drive.google.com/uc?exportview&id=1-VxQqdWhzIVt5l_7W-WljUa_iY8euUFk\"/>\n",
    "\n",
    "- Each row represents a document, and each column represents a word\n",
    "- The values in the matrix are the count of each word in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can construct a bag of words with our review data using the module [feature_extraction](https://scikit-learn.org/stable/modules/feature_extraction.html) from the Scikit-learn library\n",
    "- The [CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class constructs the bag of words for us\n",
    "\n",
    "Let us first do it for the first two reviews:\n",
    "- The `fit_transform()` method in the CountVectorizer() class first finds all the words in the documents (learn the vocabulary), and then constructs the matrix (count the words in each document):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer() #Store the class in 'count' to ease coding\n",
    "\n",
    "review_array = df['review'].values[0:2] #Take the first two reviews and store them in an array\n",
    "bag = count.fit_transform(review_array) #fit_transform takes an array as input and outputs the bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how the bag of words looks in the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1913</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>add</th>\n",
       "      <th>admit</th>\n",
       "      <th>after</th>\n",
       "      <th>against</th>\n",
       "      <th>alive</th>\n",
       "      <th>...</th>\n",
       "      <th>when</th>\n",
       "      <th>where</th>\n",
       "      <th>which</th>\n",
       "      <th>while</th>\n",
       "      <th>who</th>\n",
       "      <th>with</th>\n",
       "      <th>women</th>\n",
       "      <th>wrong</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1913  able  about  action  actor  add  admit  after  against  alive  ...  \\\n",
       "0     0     1      0       0      0    0      1      1        0      0  ...   \n",
       "1     1     0      1       2      1    1      0      0        1      1  ...   \n",
       "\n",
       "   when  where  which  while  who  with  women  wrong  you  young  \n",
       "0     0      0      1      1    0     2      1      1    2      0  \n",
       "1     1      1      0      0    1     3      0      0    1      1  \n",
       "\n",
       "[2 rows x 289 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_array = bag.toarray() #Make the bag to an array\n",
    "matrix = pd.DataFrame(data=count_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The number of times a word (/term) occurs in a document is also called the **term frequency**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## N-grams:\n",
    "\n",
    "- In our bag of words from above each term represent **one** word\n",
    "    - It is a bag of words model with **1-grams**\n",
    "- I.e., we pool all words from a document into one big bag\n",
    "    - --> we loose all information that lies in the order of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead we can specify for example 2-grams:\n",
    "    - With 1-grams: \"My name is Hjalte\" will yield the terms; 'My', 'name', 'is', 'Hjalte'\n",
    "    - With 2-grams: \"My name is Hjalte\" will yield the terms; 'My name', 'name is', 'is Hjalte'\n",
    "- N-grams of more than 1 is a way to keep some of the information in the order of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us see how to do it in Python:\n",
    "- You can choose the N-grams via the `ngram_range()` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1913 us</th>\n",
       "      <th>able to</th>\n",
       "      <th>about the</th>\n",
       "      <th>action moves</th>\n",
       "      <th>action on</th>\n",
       "      <th>actor turned</th>\n",
       "      <th>add some</th>\n",
       "      <th>admit that</th>\n",
       "      <th>after being</th>\n",
       "      <th>against his</th>\n",
       "      <th>...</th>\n",
       "      <th>with our</th>\n",
       "      <th>with some</th>\n",
       "      <th>with such</th>\n",
       "      <th>with this</th>\n",
       "      <th>women in</th>\n",
       "      <th>wrong kutcher</th>\n",
       "      <th>you go</th>\n",
       "      <th>you judge</th>\n",
       "      <th>you know</th>\n",
       "      <th>young american</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 453 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1913 us  able to  about the  action moves  action on  actor turned  \\\n",
       "0        0        1          0             0          0             0   \n",
       "1        1        0          1             1          1             1   \n",
       "\n",
       "   add some  admit that  after being  against his  ...  with our  with some  \\\n",
       "0         0           1            1            0  ...         1          0   \n",
       "1         1           0            0            1  ...         0          1   \n",
       "\n",
       "   with such  with this  women in  wrong kutcher  you go  you judge  you know  \\\n",
       "0          1          0         1              1       1          1         0   \n",
       "1          0          1         0              0       0          0         1   \n",
       "\n",
       "   young american  \n",
       "0               0  \n",
       "1               1  \n",
       "\n",
       "[2 rows x 453 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(2,2)) #Choose only 2-grams\n",
    "\n",
    "review_array = df['review'].values[0:2]\n",
    "bag = count.fit_transform(review_array)\n",
    "\n",
    "count_array = bag.toarray() #Make the bag to an array\n",
    "matrix = pd.DataFrame(data=count_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note: We will get more terms with N-grams of higher degrees\n",
    "- More terms makes the bag of words model more computationally heavy to work with\n",
    "- **Classic trade-off in text as data: Trade-off between information and computer power**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Term frequency-inverse document frequency\n",
    "\n",
    "- From the matrix above you can see that we very fast get a lot of terms even with few documents\n",
    "- It is a problem for the computational efficiency\n",
    "\n",
    "#### Is there a way to limit the terms that do not provide a lot of information?\n",
    "- The technique called: Term frequency-inverse document frequency!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Background:\n",
    "\n",
    "- When analyzing text data we often have words that appears frequently across many documents\n",
    "    - These words typically do not carry much information about each document --> they are simply just in all documents\n",
    "- Similarly there will be words that are very rare\n",
    "    - These words will carry a lot of information\n",
    "    - But the information they provide may not be enough to counteract the computational cost they carry \n",
    "--> We want to down-weight very common words and very rare words\n",
    "- That is what the term frequency - inverse document frequency (tf-idf) technique does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tf-idf:\n",
    "The tf-idf is computed like this:\n",
    "\n",
    "$tf-idf(t,d) = tf(t,d) \\times idf(t,d)$\n",
    "\n",
    "- $tf(t,d)$ is the term frequency and measures how many times a word/term $t$ occurs in a document $d$ (just as you have seen with the bag of words model)\n",
    "\n",
    "- $idf(t,d)$ is computed like this: $idf(t,d) = log \\frac{n_d}{1+df(t,d)}$\n",
    "\n",
    "    - $n_d$ is the total number of documents, and $df(t,d)$ is the number of documents $d$ that contains the term $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Very common words will have low tf-idf score because $idf(t,d)$ will be low\n",
    "- Very rare words will have low tf-idf score because $tf(t,d)$ will be low\n",
    "\n",
    "Common practice:\n",
    "- Only keep the words in a document if they have a tf-idf score above some threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we compute the tf-idf score in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer() #Ease coding\n",
    "bag_tfidf = tfidf.fit_transform(bag) #Compute the tf-idf score from the bag of words from before ('bag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1913 us</th>\n",
       "      <th>able to</th>\n",
       "      <th>about the</th>\n",
       "      <th>action moves</th>\n",
       "      <th>action on</th>\n",
       "      <th>actor turned</th>\n",
       "      <th>add some</th>\n",
       "      <th>admit that</th>\n",
       "      <th>after being</th>\n",
       "      <th>against his</th>\n",
       "      <th>...</th>\n",
       "      <th>with our</th>\n",
       "      <th>with some</th>\n",
       "      <th>with such</th>\n",
       "      <th>with this</th>\n",
       "      <th>women in</th>\n",
       "      <th>wrong kutcher</th>\n",
       "      <th>you go</th>\n",
       "      <th>you judge</th>\n",
       "      <th>you know</th>\n",
       "      <th>young american</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053631</td>\n",
       "      <td>0.053631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 453 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1913 us   able to  about the  action moves  action on  actor turned  \\\n",
       "0  0.000000  0.082333   0.000000      0.000000   0.000000      0.000000   \n",
       "1  0.053631  0.000000   0.053631      0.053631   0.053631      0.053631   \n",
       "\n",
       "   add some  admit that  after being  against his  ...  with our  with some  \\\n",
       "0  0.000000    0.082333     0.082333     0.000000  ...  0.082333   0.000000   \n",
       "1  0.053631    0.000000     0.000000     0.053631  ...  0.000000   0.053631   \n",
       "\n",
       "   with such  with this  women in  wrong kutcher    you go  you judge  \\\n",
       "0   0.082333   0.000000  0.082333       0.082333  0.082333   0.082333   \n",
       "1   0.000000   0.053631  0.000000       0.000000  0.000000   0.000000   \n",
       "\n",
       "   you know  young american  \n",
       "0  0.000000        0.000000  \n",
       "1  0.053631        0.053631  \n",
       "\n",
       "[2 rows x 453 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_array = bag_tfidf.toarray() #Make the bag to an array\n",
    "matrix_tfidf = pd.DataFrame(data=tfidf_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "matrix_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video 14.3: Text as data applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Applications (third step in our recipe)\n",
    "\n",
    "- Training a logistic model to classify whether a text is positive or negative\n",
    "- Lexicons\n",
    "- Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Applications (I/III): Training a logistic model for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall the structure of our movie review dataset:\n",
    "- Variable containing the reviews ('review')\n",
    "- Variable stating whether the person had a positive or negative sentiment towards the movie ('sentiment')\n",
    "- Variable stating whether the review is in the test or train set ('set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actor turned director bill paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i saw this film in a sneak preview and it is d...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bill paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>towards the end of the movie i felt it was too...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>this is the kind of movie that my enemies cont...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i saw descent last night at the stockholm film...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>some films that you pick up for a pound turn o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>this is one of the dumbest films ive ever seen...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment    set\n",
       "0      i went and saw this movie last night after bei...          1   test\n",
       "1      actor turned director bill paxton follows up h...          1   test\n",
       "2      as a recreational golfer with some knowledge o...          1   test\n",
       "3      i saw this film in a sneak preview and it is d...          1   test\n",
       "4      bill paxton has taken the true story of the 19...          1   test\n",
       "...                                                  ...        ...    ...\n",
       "49995  towards the end of the movie i felt it was too...          0  train\n",
       "49996  this is the kind of movie that my enemies cont...          0  train\n",
       "49997  i saw descent last night at the stockholm film...          0  train\n",
       "49998  some films that you pick up for a pound turn o...          0  train\n",
       "49999  this is one of the dumbest films ive ever seen...          0  train\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have labelled each review with a sentiment\n",
    "\n",
    "- --> We can train a machine learning model on our \"train reviews\" to predict the sentiment of our \"test reviews\"\n",
    "    - I.e., the goal is to predict the sentiment (positive or negative) of the reviews just by inputting the words in the reviews\n",
    "    \n",
    "We will use a logistic regression model for this text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we do it in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we load the train and test dataset into two different datasets:\n",
    "    - Remember that we have already cleaned the data with our cleaner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "df_train = df[df.set==\"train\"]\n",
    "df_test = df[df.set==\"test\"]\n",
    "\n",
    "# Sort the data randomly to mix positive and negative reviews\n",
    "np.random.seed(0)\n",
    "df_train = df.reindex(np.random.permutation(df_train.index))\n",
    "df_test = df.reindex(np.random.permutation(df_test.index))\n",
    "\n",
    "# Take out X and Y variable\n",
    "x_train = df_train['review'].values\n",
    "x_test = df_test['review'].values\n",
    "y_train = df_train['sentiment'].values\n",
    "y_test = df_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Second, we need to make our bag of words and down-weight common and rare words with tf-idf\n",
    "    - Remember we used `CountVectorizer` and `TfidfTransformer` to do this\n",
    "    - `TfidfVectorizer` combines the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "x_train_bag = tfidf.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Third, we fit our logistic regression model on the training set's bag of words (x_train_bag) and the true sentiments (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=0) #Text classifier\n",
    "lr.fit(x_train_bag,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fourth, we can now test our fitted logistic regression model on both the train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to make a tf-idf bag of words for the test set as well.\n",
    "# (use the transform() method for that: do NOT use fit_transform() as in the train set. Because we only use the words from the train set to fit our model on)\n",
    "x_test_bag = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9334\n",
      "Testing accuracy: 0.8844\n"
     ]
    }
   ],
   "source": [
    "# Then we predict the sentiment \n",
    "train_preds = lr.predict(x_train_bag)\n",
    "test_preds = lr.predict(x_test_bag)\n",
    "\n",
    "# And we compare the predicted sentiment with the actual sentiment\n",
    "print(\"Training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"Testing accuracy:\", np.mean([(test_preds==y_test)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can use the coefficients from the fitted model to say something about the importance of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000000000001</th>\n",
       "      <th>000001</th>\n",
       "      <th>00000110</th>\n",
       "      <th>0001</th>\n",
       "      <th>00015</th>\n",
       "      <th>001</th>\n",
       "      <th>0010</th>\n",
       "      <th>002</th>\n",
       "      <th>...</th>\n",
       "      <th>Ã©tcother</th>\n",
       "      <th>Ã©very</th>\n",
       "      <th>Ãªxtase</th>\n",
       "      <th>Ã­s</th>\n",
       "      <th>Ã­snt</th>\n",
       "      <th>Ã¸stbye</th>\n",
       "      <th>Ã¼ber</th>\n",
       "      <th>Ã¼berannoying</th>\n",
       "      <th>Ã¼berspy</th>\n",
       "      <th>Ã¼vegtigris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.008332</td>\n",
       "      <td>-0.002248</td>\n",
       "      <td>-0.036233</td>\n",
       "      <td>-0.033263</td>\n",
       "      <td>-0.006562</td>\n",
       "      <td>0.010348</td>\n",
       "      <td>-0.005333</td>\n",
       "      <td>-0.033112</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>-0.021993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034033</td>\n",
       "      <td>-0.072971</td>\n",
       "      <td>0.016622</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>-0.02997</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>-0.097527</td>\n",
       "      <td>-0.006549</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>-0.058898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 111635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00       000  0000000000001    000001  00000110      0001     00015  \\\n",
       "0 -0.008332 -0.002248      -0.036233 -0.033263 -0.006562  0.010348 -0.005333   \n",
       "\n",
       "        001      0010       002  ...  Ã©tcother     Ã©very    Ãªxtase        Ã­s  \\\n",
       "0 -0.033112 -0.001661 -0.021993  ...  0.034033 -0.072971  0.016622  0.002062   \n",
       "\n",
       "      Ã­snt    Ã¸stbye      Ã¼ber  Ã¼berannoying   Ã¼berspy  Ã¼vegtigris  \n",
       "0 -0.02997  0.010549 -0.097527     -0.006549  0.015016   -0.058898  \n",
       "\n",
       "[1 rows x 111635 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the words (features)\n",
    "features = ['_'.join(s.split()) for s in tfidf.get_feature_names_out()]\n",
    "\n",
    "# Get the coefficients from the fitted model\n",
    "coefficients = lr.coef_\n",
    "\n",
    "# Present coefficients for each feature\n",
    "coefs_df = pd.DataFrame.from_records(coefficients, columns=features)\n",
    "coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0\n",
      "great       7.554328\n",
      "excellent   6.259992\n",
      "best        5.158860\n",
      "perfect     4.730005\n",
      "wonderful   4.616599\n",
      "amazing     4.135281\n",
      "well        3.864223\n",
      "favorite    3.847079\n",
      "loved       3.829614\n",
      "love        3.820325\n",
      "fun         3.779435\n",
      "enjoyed     3.569553\n",
      "710         3.438833\n",
      "highly      3.420503\n",
      "today       3.401102\n",
      "and         3.268132\n",
      "brilliant   3.230066\n",
      "superb      3.211875\n",
      "definitely  3.089752\n",
      "still       3.051435\n"
     ]
    }
   ],
   "source": [
    "# Print the 20 words with highest positive sentiment\n",
    "print(coefs_df.T.sort_values(by=[0], ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0\n",
      "worst         -9.214507\n",
      "bad           -8.007373\n",
      "awful         -6.389001\n",
      "waste         -6.339590\n",
      "boring        -5.937048\n",
      "poor          -5.395301\n",
      "terrible      -4.865746\n",
      "nothing       -4.777008\n",
      "worse         -4.635933\n",
      "no            -4.488101\n",
      "horrible      -4.200668\n",
      "dull          -4.197008\n",
      "poorly        -4.096072\n",
      "unfortunately -3.962912\n",
      "annoying      -3.936293\n",
      "script        -3.799177\n",
      "stupid        -3.766312\n",
      "ridiculous    -3.647288\n",
      "minutes       -3.608638\n",
      "even          -3.538806\n"
     ]
    }
   ],
   "source": [
    "# Print the 20 words with lowest positive sentiment\n",
    "print(coefs_df.T.sort_values(by=[0], ascending=True).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Applications (II/III): Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sometimes we do not have labelled data as in our IMDB reviews example\n",
    "\n",
    "- I.e., we do not know in advance whether a review has a positive or negative sentiment towards a movie\n",
    "    - Recall: For each review we had a variable called 'sentiment' which stated whether the person writing the review had a positive or negative sentiment towards the movie\n",
    "- Then we cannot train a machine learning to classify the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead, we can use predefined lexicons!\n",
    "\n",
    "- The lexicons have a dictionary of words that can have some predefined labels:\n",
    "    - polarity score: positive, negative or neutral sentiment\n",
    "    - mood\n",
    "    - and so on\n",
    "\n",
    "We can use these predefined labels to score the sentiment of texts\n",
    "- The more positive words in the text, the more positive will the sentiment be\n",
    "- The more negative words in the text, the more negative will the sentiment be\n",
    "\n",
    "You can read more about lexicons [here](https://medium.com/nerd-for-tech/sentiment-analysis-lexicon-models-vs-machine-learning-b6e3af8fe746) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different lexicons:\n",
    "\n",
    "- AFINN: https://github.com/fnielsen/afinn\n",
    "- VADER: https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### AFINN:\n",
    "\n",
    "- Danish lexicon\n",
    "- Simple and popular lexicon\n",
    "- Word-list based: Contains 3382 words that are scored for polarity\n",
    "\n",
    "Positive score: Positive sentiment. Negative score: Negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### VADER:\n",
    "\n",
    "- Specifically tuned to social media\n",
    "- VADER scores both polarity and intensity of emotion\n",
    "- Word-list based as AFINN\n",
    "- But also rule-based:\n",
    "    - Example: It knows that \"dit not love\" is negative because of the negation\n",
    "    \n",
    "Positive score: Positive sentiment. Negative score: Negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work in practice?\n",
    "\n",
    "- The document is tokenized (as you know how to do know)\n",
    "- Each token in the document is matched with the words in the lexicon: Are they positive, negative or neutral?\n",
    "- All the token sentiment scores in the document are summed or averaged to predict the overall sentiment of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW:  i went and saw this movie last night after being coaxed to by a few friends of mine ill admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy i was wrong kutcher played the character of jake fischer very well and kevin costner played ben randall with such professionalism the sign of a good movie is that it can toy with our emotions this one did exactly that the entire theater which was sold out was overcome by laughter during the first half of the movie and were moved to tears during the second half while exiting the theater i not only saw many women in tears but many full grown men as well trying desperately not to let anyone see them crying this movie was great and i suggest that you go see it before you judge\n",
      "Actual Sentiment:  1\n",
      "Predicted Sentiment polarity:  -7.0\n",
      "REVIEW:  this is halfway to being a top movie the opening section which spoofs hollywood social message films is absolutely brilliant it is a riot from start to finish  the second section which introduces us to the main characters of the story is really great too we get a lot of great comic setups top notch performances and the dialog is really dynamic  spoiler warning  the one think that really annoyed me about this film though is the ending which i think contradicts everything that went before my interpretation was that this film was taking the mickey out all the silly prejudices and innuendo of small town gossip and national tabloid sensationalism i loved that the film was championing the cause that a persons sexuality is not determined by their hobbies idiosyncrasies fashion sense or whatever and then the ending goes and reenforces all the gossip and stereotypes that the movie successfully lampooned in the first place it turns out everyone was 100 right godamit this was very disappointing to what was actually a great story\n",
      "Actual Sentiment:  1\n",
      "Predicted Sentiment polarity:  16.0\n",
      "REVIEW:  christ oh christ one watches stunned incredulous and possibly deranged as this tawdry exercise in mirthless smut unfolds with all the wit and dexterity of a palsied galapagos tortoise can such things be does this movie actually exist or was i the unwitting guinea pig of some shadowy international drugs company sipping my coffee unaware that it had been spiked with a dangerous hallucinogen ive seen a lot of films and a lot of bad films but nothing prepared me for this by the end of it i was a gibbering snivelling wreck tearing at the carpet with my teeth like a dog clawing at the walls howling till my lungs were sore i pleaded desperately frenziedly for mercy to whom this appeal was made i dont know and longed with burning desire for the soothing balm of ozu yasujiro sweet weeping jesus the memories sometimes they come back to me when im at my most vulnerable when im least able to handle them i shudder i break down in tears i bite my fingernails till my hands are slathered with blood but i cant quite banish the awful flashbacks from my mind im haunted im damaged im a shell of a man  the other user comments here suggest that i am not alone in having undergone this terrifying experience which can only mean one of two things a the film does in fact exist or b i am but one victim among legions of an international conspiracy of truly sinister proportions what is quite mindboggling is that some people seem to have enjoyed their ordeal or at least have not been left traumatised by it perhaps theyre part of the operation god damn them the maniacs god damn them all to hell\n",
      "Actual Sentiment:  0\n",
      "Predicted Sentiment polarity:  -32.0\n"
     ]
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True) #Also use the emoticons in the lexicon\n",
    "review_sample=df.loc[[0,1000,49000]] #Choose some reviews from the cleaned dataset\n",
    "for i, row in review_sample.iterrows(): #Print the review, actual sentiment, and polarity score\n",
    "  print(\"REVIEW: \", row.review)\n",
    "  print(\"Actual Sentiment: \", row.sentiment)\n",
    "  print('Predicted Sentiment polarity: ', afn.score(row.review)) #Get the AFINN polarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's see how well the AFINN lexicon predicts the actual sentiment of the reviews (it takes a while to run the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = []\n",
    "for i in df['review'].values: #For each review compute the polarity score, and classify it as positive or negative\n",
    "    score = afn.score(i)\n",
    "    if score<=0:\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        preds.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71312\n"
     ]
    }
   ],
   "source": [
    "# Share of correct sentiment scores\n",
    "print(np.mean([(preds==df.sentiment.values)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/fch/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW:  i went and saw this movie last night after being coaxed to by a few friends of mine ill admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy i was wrong kutcher played the character of jake fischer very well and kevin costner played ben randall with such professionalism the sign of a good movie is that it can toy with our emotions this one did exactly that the entire theater which was sold out was overcome by laughter during the first half of the movie and were moved to tears during the second half while exiting the theater i not only saw many women in tears but many full grown men as well trying desperately not to let anyone see them crying this movie was great and i suggest that you go see it before you judge\n",
      "Actual Sentiment:  1\n",
      "Predicted Sentiment polarity:  {'neg': 0.096, 'neu': 0.765, 'pos': 0.139, 'compound': 0.734}\n",
      "REVIEW:  this is halfway to being a top movie the opening section which spoofs hollywood social message films is absolutely brilliant it is a riot from start to finish  the second section which introduces us to the main characters of the story is really great too we get a lot of great comic setups top notch performances and the dialog is really dynamic  spoiler warning  the one think that really annoyed me about this film though is the ending which i think contradicts everything that went before my interpretation was that this film was taking the mickey out all the silly prejudices and innuendo of small town gossip and national tabloid sensationalism i loved that the film was championing the cause that a persons sexuality is not determined by their hobbies idiosyncrasies fashion sense or whatever and then the ending goes and reenforces all the gossip and stereotypes that the movie successfully lampooned in the first place it turns out everyone was 100 right godamit this was very disappointing to what was actually a great story\n",
      "Actual Sentiment:  1\n",
      "Predicted Sentiment polarity:  {'neg': 0.113, 'neu': 0.718, 'pos': 0.169, 'compound': 0.9257}\n",
      "REVIEW:  christ oh christ one watches stunned incredulous and possibly deranged as this tawdry exercise in mirthless smut unfolds with all the wit and dexterity of a palsied galapagos tortoise can such things be does this movie actually exist or was i the unwitting guinea pig of some shadowy international drugs company sipping my coffee unaware that it had been spiked with a dangerous hallucinogen ive seen a lot of films and a lot of bad films but nothing prepared me for this by the end of it i was a gibbering snivelling wreck tearing at the carpet with my teeth like a dog clawing at the walls howling till my lungs were sore i pleaded desperately frenziedly for mercy to whom this appeal was made i dont know and longed with burning desire for the soothing balm of ozu yasujiro sweet weeping jesus the memories sometimes they come back to me when im at my most vulnerable when im least able to handle them i shudder i break down in tears i bite my fingernails till my hands are slathered with blood but i cant quite banish the awful flashbacks from my mind im haunted im damaged im a shell of a man  the other user comments here suggest that i am not alone in having undergone this terrifying experience which can only mean one of two things a the film does in fact exist or b i am but one victim among legions of an international conspiracy of truly sinister proportions what is quite mindboggling is that some people seem to have enjoyed their ordeal or at least have not been left traumatised by it perhaps theyre part of the operation god damn them the maniacs god damn them all to hell\n",
      "Actual Sentiment:  0\n",
      "Predicted Sentiment polarity:  {'neg': 0.214, 'neu': 0.674, 'pos': 0.112, 'compound': -0.989}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "review_sample=df.loc[[0,1000,49000]] #Choose some reviews from the cleaned dataset\n",
    "for i, row in review_sample.iterrows(): #Print the review, actual sentiment, and polarity score\n",
    "  print(\"REVIEW: \", row.review)\n",
    "  print(\"Actual Sentiment: \", row.sentiment)\n",
    "  print('Predicted Sentiment polarity: ', analyser.polarity_scores(row.review)) #Get the VADER polarity score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's see how well the VADER lexicon predicts the actual sentiment of the reviews (it takes a while to run the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in df['review'].values: #For each review compute the polarity score, and classify it as positive or negative\n",
    "    score = analyser.polarity_scores(i)[\"compound\"]\n",
    "    if score<=0:\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        preds.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69684\n"
     ]
    }
   ],
   "source": [
    "# Share of correct sentiment scores\n",
    "import numpy as np\n",
    "print(np.mean([(preds==df.sentiment.values)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Applications (III/III): Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Topic modelling is the task of assigning topics to unlabelled text documents\n",
    "\n",
    "- Our movie review example:\n",
    "    - Based on the review texts we can assign the movies into movie genres\n",
    "    - We cluster all the reviews that contains similar words\n",
    "        - For example reviews that contain words like 'horror', 'scared', 'shock', 'blood' may be clustered into the same topic: horror movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "We can make the topic modelling with the [Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2)\n",
    "\n",
    "- LDA is an unsupervised machine learning algorithm\n",
    "- Finds groups of words that appear frequently together across several documents\n",
    "    - The groups of words will then be our topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work in practice?\n",
    "\n",
    "- The LDA algorithm takes a bag of words model as input\n",
    "- It then outputs two things:\n",
    "    - a document to topic matrix (it allocates each document to a topic)\n",
    "    - a word to topic matrix (it allocates each word to a topic\n",
    "- We need to define the number of topics beforehand (the number of topics is a hyperparameter)!\n",
    "    - This is a bit arbitrary\n",
    "    - Try to play around with it and define different number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's see how it works in Python\n",
    "\n",
    "- First we need to make our bag of words:\n",
    "    - For convenience we use the built-in stop-word library in scikit-learn\n",
    "    - We set the maximum document frequency to 10 percent to exclude very common words\n",
    "    - We limit the number of words to 5000 most frequently occuring words\n",
    "        - It limits the dimensionality of the dataset to ease computation\n",
    "        \n",
    "The maximum document frequency and number of words are hyperparameters that you can tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english', max_df=0.1, max_features=5000)\n",
    "bag = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Second we fit our LDA estimator to the bag of words\n",
    "    - We specify the number of topics to 10\n",
    "    - The code may take 5-10 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,random_state=123) #The random_state parameter pass an integer that makes the result reproducible \n",
    "review_topics = lda.fit_transform(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now print the 5 most important words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "comedy black action police crime\n",
      "Topic 2:\n",
      "book version musical play role\n",
      "Topic 3:\n",
      "war american men history country\n",
      "Topic 4:\n",
      "role john performance plays actor\n",
      "Topic 5:\n",
      "dvd music video watched fun\n",
      "Topic 6:\n",
      "kids guy stupid girl school\n",
      "Topic 7:\n",
      "house horror woman dead wife\n",
      "Topic 8:\n",
      "worst minutes script awful boring\n",
      "Topic 9:\n",
      "family feel beautiful performance mother\n",
      "Topic 10:\n",
      "series original game effects action\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "word_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_): #lda.components_ stores a matrix containing the word importance for each topic\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([word_names[i]\n",
    "    for i in topic.argsort()\\\n",
    "        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on the 5 most important words we may identify following topics:\n",
    "\n",
    "1. Action and comedy movies\n",
    "2. Musicals\n",
    "3. War movies\n",
    "4. Reviews somehow related to the quality of acting (not really a movie genre)\n",
    "5. Movies from home\n",
    "6. Teen movies\n",
    "7. Horror movies\n",
    "8. Bad movies\n",
    "9. Feel-good or family movies\n",
    "10. Movies related to series"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Social Data Science: Text as Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
